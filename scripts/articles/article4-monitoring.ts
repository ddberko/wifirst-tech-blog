export const article4Content = [
  "# Comment nous monitorons 15 000 sites en temps rÃ©el avec Spot",
  "",
  "## Le problÃ¨me : pourquoi les solutions du marchÃ© ne suffisaient pas",
  "",
  "Quand vous gÃ©rez la connectivitÃ© de **15 000 sites** et **50 000 Ã©quipements rÃ©seau**, le monitoring n'est pas un dashboard qu'on consulte de temps en temps. C'est le **systÃ¨me nerveux central** de toute l'opÃ©ration.",
  "",
  "Nous avons Ã©valuÃ© les solutions du marchÃ© â€” Datadog, Zabbix, LibreNMS, PRTG, SolarWinds. Aucune ne rÃ©pondait Ã  nos besoins :",
  "",
  "| CritÃ¨re | Solutions marchÃ© | Notre besoin |",
  "|---|---|---|",
  "| Ã‰chelle | ~10K devices | 50K+ devices |",
  "| MÃ©triques/min | ~100K | **2M+** |",
  "| Latence alerting | 1-5 min | **< 30 sec** |",
  "| Contexte business | GÃ©nÃ©rique | **SpÃ©cifique Wifirst** |",
  "| CoÃ»t Ã  l'Ã©chelle | ğŸ’¸ğŸ’¸ğŸ’¸ | MaÃ®trisÃ© |",
  "| Personnalisation | LimitÃ©e | **Totale** |",
  "",
  "C'est pourquoi nous avons construit **Spot** â€” notre plateforme d'observabilitÃ© rÃ©seau, dÃ©veloppÃ©e 100% en interne.",
  "",
  "## Architecture globale",
  "",
  "```",
  "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
  "â”‚                     EDGE (par site client)                   â”‚",
  "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚",
  "â”‚  â”‚  Spot Collector (Go binary, ~15MB RAM)               â”‚   â”‚",
  "â”‚  â”‚  â”œâ”€â”€ SNMP Poller (interfaces, CPU, mem, Wi-Fi)       â”‚   â”‚",
  "â”‚  â”‚  â”œâ”€â”€ Syslog Receiver (UDP 514)                       â”‚   â”‚",
  "â”‚  â”‚  â”œâ”€â”€ Active Tester (ping, DNS, HTTP, speedtest)      â”‚   â”‚",
  "â”‚  â”‚  â”œâ”€â”€ Wi-Fi Metrics (assoc clients, RSSI, airtime)    â”‚   â”‚",
  "â”‚  â”‚  â””â”€â”€ Streaming Telemetry (gNMI/gRPC) [nouveaux AP]   â”‚   â”‚",
  "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚",
  "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
  "                                  â”‚ Protobuf / gzip",
  "                                  â”‚ via MQTT ou HTTPS",
  "                                  â–¼",
  "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
  "â”‚                      INGESTION LAYER                         â”‚",
  "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚",
  "â”‚  â”‚              Apache Kafka (3 brokers)               â”‚     â”‚",
  "â”‚  â”‚  Topics:                                            â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ raw.snmp.metrics     (~800K msg/min)           â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ raw.syslog.events    (~200K msg/min)           â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ raw.wifi.metrics     (~600K msg/min)           â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ raw.active.tests     (~100K msg/min)           â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ processed.alerts     (~2K msg/min)             â”‚     â”‚",
  "â”‚  â”‚  â””â”€â”€ processed.aggregates (~50K msg/min)            â”‚     â”‚",
  "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚",
  "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
  "                            â–¼",
  "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
  "â”‚                    PROCESSING LAYER                          â”‚",
  "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚",
  "â”‚  â”‚              Apache Flink Cluster                   â”‚     â”‚",
  "â”‚  â”‚  Jobs:                                              â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ MetricAggregator (1min, 5min, 1h rollups)     â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ AnomalyDetector (baseline dynamique)           â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ EventCorrelator (multi-source)                 â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ AlertEngine (scoring + dÃ©dup)                  â”‚     â”‚",
  "â”‚  â”‚  â”œâ”€â”€ SLACalculator (uptime, perf par client)        â”‚     â”‚",
  "â”‚  â”‚  â””â”€â”€ WiFiAnalyzer (roaming, interference, capacity) â”‚     â”‚",
  "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚",
  "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
  "                â–¼              â–¼",
  "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
  "â”‚  TimescaleDB     â”‚  â”‚  S3 (cold)       â”‚",
  "â”‚  (hot: 90 days)  â”‚  â”‚  (Parquet, 2y)   â”‚",
  "â”‚  â”œâ”€â”€ metrics     â”‚  â”‚                  â”‚",
  "â”‚  â”œâ”€â”€ events      â”‚  â”‚  Queryable via   â”‚",
  "â”‚  â”œâ”€â”€ alerts      â”‚  â”‚  Trino/Presto    â”‚",
  "â”‚  â””â”€â”€ aggregates  â”‚  â”‚                  â”‚",
  "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
  "         â”‚",
  "         â–¼",
  "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
  "â”‚                       API LAYER (Go)                         â”‚",
  "â”‚  â”œâ”€â”€ REST API (sites, devices, metrics, alerts)             â”‚",
  "â”‚  â”œâ”€â”€ GraphQL (dashboard queries)                            â”‚",
  "â”‚  â”œâ”€â”€ WebSocket (real-time updates)                          â”‚",
  "â”‚  â””â”€â”€ gRPC (internal services)                               â”‚",
  "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
  "                          â–¼",
  "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
  "â”‚                    FRONTEND (React/TypeScript)                â”‚",
  "â”‚  â”œâ”€â”€ Dashboard global (carte de France, KPIs)               â”‚",
  "â”‚  â”œâ”€â”€ Vue site (topologie, mÃ©triques, clients)               â”‚",
  "â”‚  â”œâ”€â”€ Vue device (dÃ©tail AP/switch, historique)              â”‚",
  "â”‚  â”œâ”€â”€ Alerting center (triage, acquittement, escalade)       â”‚",
  "â”‚  â””â”€â”€ Reports (SLA, capacity planning, tendances)            â”‚",
  "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
  "```",
  "",
  "## Le Spot Collector : notre agent edge",
  "",
  "Le Spot Collector est un binaire Go minimaliste qui tourne sur chaque CPE (routeur/passerelle) client. Contraintes de design :",
  "",
  "- **< 15 MB RAM** â€” les CPE ont des ressources limitÃ©es",
  "- **< 2% CPU** â€” ne pas impacter le forwarding",
  "- **RÃ©silient** â€” doit survivre aux reboots, pertes de connectivitÃ©",
  "- **Auto-updateable** â€” via notre pipeline de dÃ©ploiement Ansible",
  "",
  "```go",
  "// Extrait simplifiÃ© du collector (Go)",
  "type Collector struct {",
  "    pollers    []Poller",
  "    buffer     *RingBuffer    // Buffer local en cas de perte de connectivitÃ©",
  "    transport  Transport      // MQTT ou HTTPS selon le site",
  "    compress   bool           // Protobuf + gzip",
  "}",
  "",
  "func (c *Collector) Run(ctx context.Context) error {",
  "    ticker := time.NewTicker(60 * time.Second)",
  "    defer ticker.Stop()",
  "    ",
  "    for {",
  "        select {",
  "        case <-ticker.C:",
  "            metrics := c.collectAll()",
  "            ",
  "            batch := &MetricBatch{",
  "                SiteID:    c.siteID,",
  "                Timestamp: time.Now().UnixMilli(),",
  "                Metrics:   metrics,",
  "            }",
  "            ",
  "            encoded, _ := proto.Marshal(batch)",
  "            compressed := gzip.Compress(encoded)",
  "            ",
  "            if err := c.transport.Send(compressed); err != nil {",
  "                // Perte de connectivitÃ© â†’ buffer local",
  "                c.buffer.Push(compressed)",
  '                log.Warn("buffered locally", "size", c.buffer.Len())',
  "            }",
  "            ",
  "        case <-ctx.Done():",
  "            return c.flush() // Envoyer le buffer restant",
  "        }",
  "    }",
  "}",
  "```",
  "",
  "### Gestion de la perte de connectivitÃ©",
  "",
  "Un point crucial : nos collectors tournent sur des sites dont **la connectivitÃ© peut tomber**. Le ring buffer local stocke jusqu'Ã  4h de mÃ©triques. Ã€ la reconnexion, les donnÃ©es sont renvoyÃ©es avec leur timestamp original â€” pas de trou dans les graphes.",
  "",
  "```",
  "Timeline de perte de connectivitÃ© :",
  "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
  "[10:00] Collecte normale â†’ envoi Kafka âœ…",
  "[10:01] Collecte normale â†’ envoi Kafka âœ…",
  "[10:02] WAN DOWN â†’ buffer local ğŸ’¾",
  "[10:03] buffer local ğŸ’¾ (2 batches)",
  "[10:04] buffer local ğŸ’¾ (3 batches)",
  "...",
  "[11:15] WAN UP â†’ flush buffer (73 batches) â†’ Kafka âœ…",
  "[11:16] Collecte normale â†’ envoi Kafka âœ…",
  "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
  "RÃ©sultat : zÃ©ro perte de donnÃ©es, graphes continus",
  "```",
  "",
  "## Le moteur d'alerting intelligent",
  "",
  "### Le problÃ¨me du bruit",
  "",
  "Avec 50 000 Ã©quipements et des seuils classiques, on obtenait **~3 000 alertes/jour**. Impossible Ã  traiter pour une Ã©quipe NOC de 8 personnes. 90% Ã©taient du bruit.",
  "",
  "### Notre approche : le scoring multi-facteurs",
  "",
  "Chaque Ã©vÃ©nement passe par un **moteur de scoring** qui pondÃ¨re plusieurs dimensions :",
  "",
  "```",
  "Score final = Î£ (facteur Ã— poids)",
  "",
  "Facteurs :",
  "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
  "SÃ©vÃ©ritÃ© technique        â”‚ Poids: 0.25",
  "  - AP down = 80          â”‚",
  "  - Interface flapping = 60â”‚",
  "  - CPU > 90% = 40        â”‚",
  "                           â”‚",
  "Impact business            â”‚ Poids: 0.35",
  "  - Nb utilisateurs       â”‚",
  "    affectÃ©s (0-100)       â”‚",
  "  - Type de site (VIP=2x) â”‚",
  "                           â”‚",
  "Contexte temporel          â”‚ Poids: 0.15",
  "  - Heure ouvrable = 100  â”‚",
  "  - Nuit = 30             â”‚",
  "  - Maintenance = 0       â”‚",
  "                           â”‚",
  "Historique                 â”‚ Poids: 0.15",
  "  - Site stable = 100     â”‚",
  "  - Site instable = 40    â”‚",
  "  - DÃ©jÃ  alertÃ© = 10      â”‚",
  "                           â”‚",
  "CorrÃ©lation                â”‚ Poids: 0.10",
  "  - Ã‰vÃ©nement isolÃ© = 80  â”‚",
  "  - Ã‰vÃ©nement corrÃ©lÃ©     â”‚",
  "    (panne opÃ©rateur) = 20 â”‚",
  "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
  "",
  "Seuils :",
  "  Score > 70 â†’ P1 (page immÃ©diate)",
  "  Score > 50 â†’ P2 (ticket urgent)",
  "  Score > 30 â†’ P3 (ticket normal)",
  "  Score < 30 â†’ Log only (pas d'alerte)",
  "```",
  "",
  "#### Exemple concret : la mÃªme panne, deux scores diffÃ©rents",
  "",
  "```",
  "Ã‰vÃ©nement : AP down",
  "",
  "ScÃ©nario A â€” HÃ´tel 5â˜… Paris, 14h, 45 clients connectÃ©s, site stable",
  "  SÃ©vÃ©ritÃ©:    80 Ã— 0.25 = 20.0",
  "  Impact:      90 Ã— 0.35 = 31.5  (45 clients, VIP)",
  "  Temporel:   100 Ã— 0.15 = 15.0  (heure ouvrable)",
  "  Historique: 100 Ã— 0.15 = 15.0  (site stable)",
  "  CorrÃ©lation: 80 Ã— 0.10 =  8.0  (isolÃ©)",
  "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
  "  TOTAL = 89.5 â†’ P1 ğŸ”´ Page immÃ©diate",
  "",
  "ScÃ©nario B â€” Camping fermÃ©, 3h du matin, 0 clients, site instable",
  "  SÃ©vÃ©ritÃ©:    80 Ã— 0.25 = 20.0",
  "  Impact:       5 Ã— 0.35 =  1.75 (0 clients, non-VIP)",
  "  Temporel:    30 Ã— 0.15 =  4.5  (nuit)",
  "  Historique:  40 Ã— 0.15 =  6.0  (instable)",
  "  CorrÃ©lation: 80 Ã— 0.10 =  8.0  (isolÃ©)",
  "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
  "  TOTAL = 40.25 â†’ P3 ğŸ“‹ Ticket normal",
  "```",
  "",
  "**MÃªme Ã©vÃ©nement technique, rÃ©ponse totalement diffÃ©rente.** C'est Ã§a, le contexte business.",
  "",
  "### RÃ©sultat : -90% de bruit",
  "",
  "```",
  "Alertes par jour (mÃ©diane glissante 30 jours) :",
  "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
  "Avant scoring : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  3 200",
  "AprÃ¨s scoring : â–ˆâ–ˆâ–ˆ                            320",
  "  dont P1     : â–ˆ                               42",
  "  dont P2     : â–ˆ                               98",
  "  dont P3     : â–ˆ                              180",
  "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
  "Taux de faux positifs : 8% (vs 91% avant)",
  "```",
  "",
  "## TimescaleDB : le choix du stockage time-series",
  "",
  "### Pourquoi TimescaleDB plutÃ´t que InfluxDB ou Prometheus ?",
  "",
  "| CritÃ¨re | TimescaleDB | InfluxDB | Prometheus |",
  "|---|---|---|---|",
  "| Langage de requÃªte | **SQL** (natif) | InfluxQL/Flux | PromQL |",
  "| Compression | Excellente | Bonne | TrÃ¨s bonne |",
  "| Joins avec data relationnelle | âœ… natif | âŒ | âŒ |",
  "| Continuous aggregates | âœ… matÃ©rialisÃ©s | LimitÃ© | Recording rules |",
  "| RÃ©tention flexible | âœ… par hypertable | âœ… | âœ… |",
  "| Ã‰cosystÃ¨me | PostgreSQL | PropriÃ©taire | CNCF |",
  "| Haute dispo | Patroni/streaming | Enterprise $$ | Thanos/Cortex |",
  "",
  "Le choix de TimescaleDB s'est imposÃ© pour une raison principale : **c'est du PostgreSQL**. Notre Ã©quipe connaÃ®t SQL, nos outils parlent SQL, et pouvoir JOINer des mÃ©triques avec des donnÃ©es de notre CMDB (sites, clients, contrats) dans la mÃªme requÃªte est un game-changer.",
  "",
  "```sql",
  "-- Exemple : sites avec le plus de dÃ©gradation Wi-Fi cette semaine",
  "-- corrÃ©lÃ© avec les donnÃ©es client de Netbox",
  "SELECT ",
  "    s.name AS site_name,",
  "    s.client_name,",
  "    s.contract_type,",
  "    AVG(m.value) AS avg_airtime_pct,",
  "    COUNT(DISTINCT m.device_id) AS affected_aps,",
  "    s.total_users",
  "FROM metrics m",
  "JOIN sites s ON m.site_id = s.id",
  "WHERE m.metric_name = 'wifi.airtime.utilization'",
  "  AND m.time > NOW() - INTERVAL '7 days'",
  "  AND m.value > 80  -- airtime > 80% = problÃ¨me",
  "GROUP BY s.id, s.name, s.client_name, s.contract_type, s.total_users",
  "HAVING COUNT(DISTINCT m.device_id) > 2  -- au moins 3 AP touchÃ©s",
  "ORDER BY s.total_users DESC  -- prioriser par impact",
  "LIMIT 20;",
  "```",
  "",
  "### SchÃ©ma et compression",
  "",
  "```sql",
  "-- Hypertable principale",
  "CREATE TABLE metrics (",
  "    time        TIMESTAMPTZ NOT NULL,",
  "    site_id     INT NOT NULL,",
  "    device_id   INT NOT NULL,",
  "    metric_name TEXT NOT NULL,",
  "    value       DOUBLE PRECISION,",
  "    tags        JSONB",
  ");",
  "",
  "SELECT create_hypertable('metrics', 'time',",
  "    chunk_time_interval => INTERVAL '1 day');",
  "",
  "-- Compression aprÃ¨s 7 jours",
  "ALTER TABLE metrics SET (",
  "    timescaledb.compress,",
  "    timescaledb.compress_segmentby = 'site_id, device_id, metric_name',",
  "    timescaledb.compress_orderby = 'time'",
  ");",
  "",
  "SELECT add_compression_policy('metrics', INTERVAL '7 days');",
  "",
  "-- Continuous aggregate pour les dashboards",
  "CREATE MATERIALIZED VIEW metrics_hourly",
  "WITH (timescaledb.continuous) AS",
  "SELECT ",
  "    time_bucket('1 hour', time) AS bucket,",
  "    site_id, device_id, metric_name,",
  "    AVG(value) AS avg_val,",
  "    MAX(value) AS max_val,",
  "    MIN(value) AS min_val,",
  "    COUNT(*) AS sample_count",
  "FROM metrics",
  "GROUP BY bucket, site_id, device_id, metric_name;",
  "```",
  "",
  "**Taux de compression moyen : 15:1** â€” 2 To de donnÃ©es brutes â†’ 130 Go compressÃ©es.",
  "",
  "## Ce qu'on a appris en 3 ans",
  "",
  "### 1. Les mÃ©triques brutes ne suffisent pas",
  "",
  "Un graphe CPU Ã  95% ne dit rien si on ne sait pas qu'il s'agit d'un switch qui sert 200 utilisateurs dans un hÃ´tel VIP pendant un congrÃ¨s. **Le contexte business est roi.**",
  "",
  "### 2. L'observabilitÃ©, ce n'est pas que des mÃ©triques",
  "",
  "Spot ingÃ¨re 3 types de signaux :",
  "",
  "```",
  "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”",
  "â”‚           Les 3 piliers de l'observabilitÃ©   â”‚",
  "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤",
  "â”‚   MÃ©triques   â”‚    Logs      â”‚   Traces     â”‚",
  "â”‚               â”‚              â”‚              â”‚",
  "â”‚ SNMP, gNMI    â”‚ Syslog       â”‚ Flux rÃ©seau  â”‚",
  "â”‚ Wi-Fi stats   â”‚ Auth logs    â”‚ Transactions â”‚",
  "â”‚ Active tests  â”‚ Config diff  â”‚ API calls    â”‚",
  "â”‚               â”‚              â”‚              â”‚",
  "â”‚ â†’ TimescaleDB â”‚ â†’ Loki       â”‚ â†’ Tempo      â”‚",
  "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
  "```",
  "",
  "La corrÃ©lation entre ces 3 types est ce qui permet de passer de \"quelque chose ne va pas\" Ã  \"voici exactement ce qui s'est passÃ©\".",
  "",
  "### 3. Investir dans le self-healing",
  "",
  "L'Ã©tape suivante aprÃ¨s la dÃ©tection, c'est la remÃ©diation automatique. Aujourd'hui, **35% de nos incidents P3** sont rÃ©solus automatiquement sans intervention humaine :",
  "",
  "```",
  "Actions de remÃ©diation automatiques :",
  "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€",
  "AP non-rÃ©pondant     â†’ Reboot distant via SNMP/SSH",
  "Port en err-disable  â†’ Bounce automatique + ticket",
  "Client en boucle authâ†’ Clear session RADIUS + re-auth",
  "Canal Wi-Fi saturÃ©   â†’ Changement canal automatique",
  "Lien WAN dÃ©gradÃ©     â†’ Bascule backup + notification",
  "```",
  "",
  "### 4. Le coÃ»t de la dette technique",
  "",
  "Spot a 3 ans. Certains choix initiaux nous rattrapent :",
  "",
  "- Le premier schÃ©ma TimescaleDB n'avait pas de partitionnement par site â†’ requÃªtes lentes sur les gros clients. MigrÃ© en 2024.",
  "- Le format de sÃ©rialisation Ã©tait JSON au dÃ©but â†’ passage Ã  Protobuf a rÃ©duit la bande passante de 60%.",
  "- Le frontend initial en Vue.js a Ã©tÃ© rÃ©Ã©crit en React pour s'aligner avec le reste de l'Ã©quipe.",
  "",
  "**LeÃ§on** : les choix d'architecture initiaux ont un impact Ã©norme Ã  l'Ã©chelle. Prenez le temps de bien les faire.",
  "",
  "## Chiffres clÃ©s de Spot en production",
  "",
  "| MÃ©trique | Valeur |",
  "|---|---|",
  "| Sites monitorÃ©s | 15 200 |",
  "| Devices monitorÃ©s | 52 400 |",
  "| MÃ©triques ingÃ©rÃ©es/min | 2,1 millions |",
  "| Events/jour | 45 millions |",
  "| Latence P99 ingestion | 850 ms |",
  "| Latence P99 alerting | 12 sec |",
  "| DisponibilitÃ© Spot (12 mois) | 99,97% |",
  "| Stockage hot (90j) | 8,2 To |",
  "| Stockage cold (2 ans) | 42 To |",
  "| CoÃ»t infra mensuel | ~4 200â‚¬ |",
  "",
  "Ã€ comparer avec une estimation Datadog pour le mÃªme volume : **~35 000â‚¬/mois**. Le build vs buy est clairement en notre faveur Ã  cette Ã©chelle.",
  "",
  "## What's next",
  "",
  "Les prochaines Ã©volutions de Spot :",
  "",
  "- **AIOps** â€” DÃ©tection d'anomalies par ML (isolation forest, LSTM pour les sÃ©ries temporelles)",
  "- **Predictive maintenance** â€” Anticiper les pannes de hardware avant qu'elles surviennent",
  "- **Root Cause Analysis automatisÃ©e** â€” Graphe de dÃ©pendances + propagation causale",
  "- **OpenTelemetry** â€” Migration progressive vers les standards OTel pour l'instrumentation",
  "",
  "---",
  "",
  "*Article rÃ©digÃ© par David Berkowicz, CTO Wifirst â€” Juin 2025*",
].join("\\n");
